import whisperx
from whisperx import load_align_model, align
from pyannote.audio import Pipeline
import torch


def transcribe_with_whisperx(audio_path: str, model_size: str = "large-v3", language: str = "ru", device: str = "cuda"):

    import torch
    print(f"[üß†] torch.cuda.is_available() = {torch.cuda.is_available()}")
    print(f"[üß†] torch version: {torch.__version__}")
    print(f"[üß†] CUDA device: {torch.cuda.get_device_name(0)}")

    print(f"[üîç] –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å: {model_size} –Ω–∞ {device}...")
    model = whisperx.load_model(model_size, device, language=language)

    print(f"[üéß] –†–∞—Å–ø–æ–∑–Ω–∞—ë–º –∞—É–¥–∏–æ: {audio_path}...")
    result = model.transcribe(audio_path)

    print("[üìå] –ë–∞–∑–æ–≤–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è:")
    for segment in result["segments"]:
        print(f"[{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}")

    # print("\n[üîÅ] –ó–∞–ø—É—Å–∫–∞–µ–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø–æ —Å–ª–æ–≤–∞–º...")
    # model_a, metadata = whisperx.load_align_model(language_code=language, device=device)
    # result_aligned = whisperx.align(result["segments"], model_a, metadata, audio_path, device)
    #
    # print("\n[‚úÖ] –í—ã—Ä–∞–≤–Ω–µ–Ω–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã:")
    # for seg in result_aligned["segments"]:
    #     words = " ".join([w["word"] for w in seg["words"]])
    #     print(f"[{seg['start']:.2f} - {seg['end']:.2f}] {words}")

    return result


def transcribe_with_whisperx_diarization(audio_path, hf_token, min_speakers=1, max_speakers=10, device="cuda"):
    # 1. –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è
    model = whisperx.load_model("large-v3", device)
    result = model.transcribe(audio_path)

    # 2. Diarization —á–µ—Ä–µ–∑ Pyannote
    pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization-3.1", use_auth_token=hf_token)
    diar = pipeline(audio_path, min_speakers=min_speakers, max_speakers=max_speakers)

    diarized = []
    for seg in result["segments"]:
        speaker = "unknown"
        for turn, _, label in diar.itertracks(yield_label=True):
            if seg["start"] < turn.end and seg["end"] > turn.start:
                speaker = label
                break
        seg["speaker"] = speaker
        diarized.append(seg)
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç diarization: {diarized}")

    aligned_model, metadata = whisperx.load_align_model(language_code="ru", device=device)
    aligned = align(diarized, aligned_model, metadata, audio_path, device=device)
    return aligned


from diarizen.pipelines.inference import DiariZenPipeline
from your_asr_module import transcribe_with_whisperx  # –≤–∞—à–∞ ASR
from llm_client import LLMClient  # –≤–∞—à –∫–ª–∏–µ–Ω—Ç –¥–ª—è call LLM
import json

# 1. –ó–∞–≥—Ä—É–∂–∞–µ–º DiariZen pipeline
diar_pipeline = DiariZenPipeline.from_pretrained("BUT-FIT/diarizen-meeting-base")

# 2. –§—É–Ω–∫—Ü–∏—è LLM-–∫–æ—Ä—Ä–µ–∫—Ü–∏–∏
PROMPT_TEMPLATE = """
–ò—Å–ø—Ä–∞–≤—å –º–µ—Ç–∫–∏ –≥–æ–≤–æ—Ä—è—â–∏—Ö, –µ—Å–ª–∏ –µ—Å—Ç—å –æ—à–∏–±–∫–∏:

{turns}

–û—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
speaker_id <–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –Ω–æ–º–µ—Ä>, start, end
...
"""


def correct_diarization_with_llm(asr_segments, speaker_labels, llm_client):
    # –°–æ–±–∏—Ä–∞–µ–º turns
    lines = []
    for seg, sp in zip(asr_segments, speaker_labels):
        text = seg["text"].strip()
        lines.append(f"{sp}: {text}")
    prompt = PROMPT_TEMPLATE.format(turns="\n".join(lines))
    resp = llm_client.generate(prompt)
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–≤–µ—Ç
    corrected = {}
    for line in resp.splitlines():
        parts = line.split(',')
        sid, start, end = parts[0].strip(), float(parts[1]), float(parts[2])
        corrected[(start, end)] = sid
    return corrected


def diarize_and_correct(audio_path, hf_token, llm_client):
    # ASR + raw diarization
    asr = transcribe_with_whisperx(audio_path, device="cuda")
    diar = diar_pipeline(audio_path)

    segments, labels = [], []
    for seg, _, sp in diar.itertracks(yield_label=True):
        segments.append({"start": seg.start, "end": seg.end})
        labels.append(f"speaker_{sp}")

    # LLM-–∫–æ—Ä—Ä–µ–∫—Ü–∏—è
    corrected = correct_diarization_with_llm(segments, labels, llm_client)

    # –ú–µ—Ä–∂–∏–º: –∑–∞–º–µ–Ω—è–µ–º speaker –≤ asr["segments"]
    new_segments = []
    for seg in asr["segments"]:
        for (s, e), sid in corrected.items():
            if seg["start"] < e and seg["end"] > s:
                seg["speaker"] = sid
                break
        new_segments.append(seg)
    return {"segments": new_segments}
