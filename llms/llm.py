import requests
import json

class LocalLLMClient:
    def __init__(self, host="http://localhost:11434", model="qwen3"):
        self.host = host
        self.model = model

    def generate_answer_stream(self, prompt):
        """
        –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç prompt –≤ Ollama (stream=True) –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å —á–∞—Å—Ç—è–º–∏ –æ—Ç–≤–µ—Ç–∞.
        """
        url = f"{self.host}/api/generate"
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": True
        }
        # stream=True –ø–æ–∑–≤–æ–ª—è–µ—Ç —á–∏—Ç–∞—Ç—å –ø–æ —á–∞—Å—Ç—è–º (Response.iter_lines)
        with requests.post(url, json=payload, stream=True, timeout=180) as resp:
            resp.raise_for_status()
            for line in resp.iter_lines(decode_unicode=True):
                if line:
                    try:
                        data = json.loads(line)
                        # Ollama –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–∫–µ–Ω—ã –≤ –ø–æ–ª–µ "response"
                        # –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª–∏ ‚Äî –≤ "message"
                        chunk = data.get("response") or data.get("message")
                        if chunk:
                            yield chunk
                    except Exception as e:
                        print(f"[Ollama Stream] –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")

    # –û–±—ã—á–Ω—ã–π (–Ω–µ –ø–æ—Ç–æ–∫–æ–≤—ã–π) –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏

    def generate_answer(self, prompt):
        url = f"{self.host}/api/generate"
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": False
        }
        resp = requests.post(url, json=payload, timeout=180)
        resp.raise_for_status()
        data = resp.json()
        return data.get("response") or data.get("message") or str(data)

def load_llm_client(host="http://localhost:11434", model="qwen3"):
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ LLM (–≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑)
    """
    print(f"‚è≥ –ó–∞–ø—É—Å–∫ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ LLM-–∫–ª–∏–µ–Ω—Ç–∞ ({model}, {host})...")
    return LocalLLMClient(host=host, model=model)


# from ollama import Client
#
# client = Client(host='http://localhost:11434')
#
# def analyze_transcript_qwen3(transcript_text):
#     print("‚è≥ [Qwen3] –ù–∞—á–∏–Ω–∞—é –∞–Ω–∞–ª–∏–∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞...")
#     print(f"üî∏ –î–ª–∏–Ω–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞: {len(transcript_text)} —Å–∏–º–≤–æ–ª–æ–≤")
#     print(f"üî∏ –ü—Ä–æ–º–ø—Ç –¥–ª—è LLM (–Ω–∞—á–∞–ª–æ): {PROMPT[:200]}...")
#
#     prompt_to_send = PROMPT.format(transcript_text=transcript_text)
#     print(f"üîπ –§—Ä–∞–≥–º–µ–Ω—Ç prompt –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ (–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤):\n{prompt_to_send[:500]}...")
#
#     try:
#         response = client.generate(
#             model='qwen3',
#             prompt=prompt_to_send,
#             stream=False
#         )
#     except Exception as e:
#         print(f"‚ùå [Qwen3] –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ LLM: {e}")
#         raise
#
#     print("‚úÖ [Qwen3] –ú–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –æ—Ç–≤–µ—Ç. –î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞:", len(response['response']))
#     print("üîπ –ü—Ä–µ–≤—å—é –æ—Ç–≤–µ—Ç–∞ (–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤):\n", response['response'][:500], "...")
#
#     return response['response']

